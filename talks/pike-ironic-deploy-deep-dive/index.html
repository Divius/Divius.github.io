<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Ironic Deploy Deep Dive (Pike Version)</title>
                <meta name="author" content="Dmitry Tantsur (divius.inside@gmail.com)">

		<link rel="stylesheet" href="../reveal.js/css/reveal.css">
		<link rel="stylesheet" href="../reveal.js/css/theme/dtantsur.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi) ? '../reveal.js/css/print/pdf.css' : '../reveal.js/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">


<section>
    <h1 class="title">Ironic Deploy Deep Dive</h1>
    <h2>The Pike Release Version</h2>
    <br><br>
    <p><small>Dmitry Tantsur (Principal Software Engineer, Red Hat)</small>
    <br>
    <small>
        <a href="https://dtantsur.github.io/talks/pike-ironic-deploy-deep-dive/">
            dtantsur.github.io/talks/pike-ironic-deploy-deep-dive</a>
    </small></p>
</section>

<section>
    <h1>Agenda</h1>
    <ul>
        <li>Overview of Ironic drivers</li>
        <li>Scheduling on bare metal nodes</li>
        <li>Initiating the deployment process</li>
        <li>The iSCSI deploy interface</li>
        <li>Boot management and PXE boot</li>
        <li>Connecting networks and VIFs</li>
    </ul>
    <p><small>Tear down is covered by another deep dive.</small></p>
</section>

<section>

<section>
    <h1 class="title">Ironic drivers overview</h1>
</section>

<section>
    <h1>Ironic drivers overview</h1>
    <h2>Driver interfaces</h2>
    <p>Most of the deployment actions are done by drivers. The drivers, in turn,
    consist of <em>interfaces</em>, each with a different role in hardware
    management.</p>
</section>

<section>
    <h1>Ironic drivers overview</h1>
    <h2>Power and management</h2>
    <p>The <em>power</em> interface handles powering nodes on and off</p>
    <p>The <em>management</em> interface handles additional vendor-specific
    actions, like getting or setting boot device.</p>
    <p>These drivers are tied to the protocol to access the BMC</p>
    <p>Examples include ipmitool, redfish, ilo.</p>
</section>

<section>
    <h1>Ironic drivers overview</h1>
    <h2>Boot and deploy</h2>
    <p>The <em>boot</em> interface handles how either the deployment ramdisk or
    the final instance get booted on the node. Examples include PXE/iPXE and
    vendor-specific virtual media approaches.</p>
    <p>The <em>deploy</em> interface orchestrates the deployment process,
    including how exactly an image gets transferred to a node. Currently
    supported are iSCSI-based and direct deploy methods.</p>
</section>

<section>
    <h1>Ironic drivers overview</h1>
    <h2>Network</h2>
    <p>The <em>network</em> interface handles how networks are connected to
    and disconnected from a node</p>
    <p>Currently supported are the following implementations:
    <ul>
        <li><em>none</em> networking does nothing, and expects DHCP to be
            configured externally.</li>
        <li><em>flat</em> networking uses Neutron with one flat provision
            network, also serving as a tenant network.</li>
        <li><em>neutron</em> networking also uses Neutron, but it is able to
            talk to switch-specific ML2 drivers to connect/disconnect different
            networks to and from nodes.</li>
    </ul>
</section>

<section>
    <h1>Ironic drivers overview</h1>
    <h2>Classic drivers</h2>
    <p>Before the Ocata release, the drivers were always monolithic. All
    interfaces were hardcoded, and partly reflected in their names.</p>
    <ul>
        <li>The <var>pxe_ipmitool</var> driver uses <var>ipmitool</var>
            <em>power</em> and <em>management</em>, <var>pxe</var> boot and
            iSCSI-based <em>deploy</em> as performed by IPA.</li>
        <li>The <var>agent_ipmitool</var> driver uses <var>ipmitool</var>
            <em>power</em> and <em>management</em>, <var>pxe</var> boot and
            direct <em>deploy</em> as performed by IPA.</li>
        <li><small>We are not good at naming, are we?</small></li>
    </ul>
    <p><a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/ipmi.py#L51-L70">
        ironic/drivers/ipmi.py</a></p>
</section>

<section>
    <h1>Ironic drivers overview</h1>
    <h2>Classic drivers drawbacks</h2>
    <p>The problem is: as of Pike-3 we will have <strong>11</strong> hardware
    interfaces and <strong>36</strong> drivers.</p>
    <p>Every time we add a new implementation for any interface, we need more
    drivers.</p>
    <p>The crisis burst out when we introduced the <em>network</em> interface,
    all implementations of which were compatible with all drivers.</p>
    <p>Starting with Ocata, we support a new concept of <em>dynamic drivers
    </em>.</p>
</section>

<section>
    <h1>Ironic drivers overview</h1>
    <h2>Hardware types</h2>
    <p>A <em>hardware type</em> defines which interface implementations it is
    compatible with, and which priority they have.</p>
    <p>New fields were provided on nodes for each of the interface. E.g.
    <var>boot_interface</var>, <var>power_interface</var>, etc.</p>
    <p>A <em>dynamic driver</em> is built on fly, based on these fields,
    interface implementations enabled in the configuration, and the defaults
    from the node's <em>hardware type</em>.</p>
    <p><a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/ipmi.py#L27-L48">
        ironic/drivers/ipmi.py</a>
    <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/generic.py#L31-L66">
        ironic/drivers/generic.py</a></p>
    <p><a href="https://docs.openstack.org/project-install-guide/baremetal/draft/enrollment.html#defaults-for-hardware-interfaces">
        install-guide on interfaces</a></p>
</section>

</section>
<section>

<section>
    <h1 class="title">Scheduling on bare metal</h1>
</section>

<section>
    <h1>Scheduling on bare metal</h1>
    <h2>Exposing resources - before Pike</h2>
    <p>Historically, we've been exposing bare metal resources similar to the
    way virtual resources are exposed:
    <a href="https://github.com/openstack/nova/blob/3284851437e24250d46edba20789a2e5f1f435a0/nova/virt/ironic/driver.py#L261-L325">
        nova/virt/ironic/driver.py</a>.
    </p>
    <p>For example, a node with 16 GiB of RAM and 4 CPUs was represented as a
    hypervisor with 16 GiB of RAM and 4 CPUs.</p>
    <p>When <strong>any</strong> instance is deployed on it, it's reported as
    a hypervisor with 16 GiB RAM and 4 CPUs <strong>used</strong>.</p>
</section>

<section>
    <h1>Scheduling on bare metal</h1>
    <h2>Exact filters</h2>
    <p>This approach is racy. If a users asks for 2 instances with 2 GiB of RAM
    and one 1 CPU each, the scheduler can try placing both of them on the same
    bare metal node. Only one of the attempts will succeed.</p>
    <ul>
        <li>One mitigation is to use <em>exact scheduling filters</em>.</li>
        <li>Another is to have a lot of retries in the
            <var>RetryFilter</var>. TripleO uses <strong>30</strong>.
            It works, but has a strong negative impact when an actual
            failure happens.</li>
    </ul>
</section>

<section>
    <h1>Scheduling on bare metal</h1>
    <h2>Exposing resources - Pike</h2>
    <p>Now every node exposes several <em>resource classes</em> to the
    scheduler:
    <a href="https://github.com/openstack/nova/blob/6c3520ac5ba789dd40d51c0d20a30d0dc44a8c07/nova/virt/ironic/driver.py#L632-L671">
        nova/virt/ironic/driver.py</a>.
    </p>
    <p>This includes traditional memory/disk/CPU resources, as well as
    baremetal-specific <em>custom resource class</em>, fetches from a node's
    <var>resource_class</var> field.</p>
</section>

<section>
    <h1>Scheduling on bare metal</h1>
    <h2>Exposing resources - After Pike</h2>
    <p>At some point in time bare metal nodes will stop expose memory/disk/CPU
    resources to the scheduler completely.</p>
    <p>Flavors targeting bare metal will have to request a custom resource
    class instead of them:
    <a href="https://docs.openstack.org/project-install-guide/baremetal/draft/configure-nova-flavors.html#scheduling-based-on-resource-classes">
        install-guide</a>.
    </p>
</section>

<section>
    <h1>Scheduling on bare metal</h1>
    <h2>Capabilities</h2>
    <p><em>Capabilities</em> allow picking nodes based on non-standard
    properties. Nowadays they are partly replaced by <em>custom resource
    classes</em>, but can still be useful.</p>
    <p>A flavor can have capabilities requested via its <var>extra_specs</var>.
    They will be matched against capabilities as reported by the Ironic Nova
    driver: <a href="https://github.com/openstack/nova/blob/6c3520ac5ba789dd40d51c0d20a30d0dc44a8c07/nova/virt/ironic/driver.py#L286-L299">
        nova/virt/ironic/driver.py</a></p>
</section>

</section>
<section>

<section>
    <h1 class="title">Initiating the deployment</h1>
</section>

<section>
    <h1>Initiating the deployment</h1>
    <h2>High-level overview</h2>
    <p><a href="https://github.com/openstack/nova/blob/6c3520ac5ba789dd40d51c0d20a30d0dc44a8c07/nova/virt/ironic/driver.py#L856-L964">
        nova/virt/ironic/driver.py</a></p>
    <ol>
        <li>Add <var>instance_info</var> to the node</li>
        <li>Add <var>instance_uuid</var> to lock the node</li>
        <li>Validate the final node information</li>
        <li>Plug VIFs and start the firewall</li>
        <li>Build and store a config drive</li>
        <li>Issue a provisioning request</li>
        <li>Wait for the inevitable success</li>
    </ol>
</section>

<section>
    <h1>Initiating the deployment</h1>
    <h2>Instance information</h2>
    <p><a href="https://github.com/openstack/nova/blob/6c3520ac5ba789dd40d51c0d20a30d0dc44a8c07/nova/virt/ironic/patcher.py">
        nova/virt/ironic/patcher.py</a></p>
    <ul>
        <li>Image information: Glance source, disk, swap and ephemeral
            partition sizes</li>
        <li>Nova host ID (used when binding ports)</li>
        <li>Flavor details: VCPUs, memory, disk (will probably go away
            eventually)</li>
        <li>Requested (and matched capabilities)</li>
    </ul>
</section>

<section>
    <h1>Initiating the deployment</h1>
    <h2>Instance UUID</h2>
    <p>The <var>instance_uuid</var> field is used to lock the chosen node.</p>
    <p>Before this point, the node picked by the scheduler can still be used
    by anything else. This is where potential races can happen.</p>
    <p>The <var>instance_uuid</var> field can only be added or removed,
    but it's not possible to replace an existing value.</p>
    <p>So after it's successfully set to an instance UUID, Nova is safe
    to proceed with deploying on it.</p>
</section>

<section>
    <h1>Initiating the deployment</h1>
    <h2>Plugging VIFs</h2>
    <p>Ironic has to know VIF IDs to be able to talk to Neutron.</p>
    <p>Previously, they were passed via <var>extra[vif_port_id]</var>, now
    we have a separate API for that.</p>
    <p>Nova requests every VIF to be plugged:
    <a href="https://github.com/openstack/nova/blob/6c3520ac5ba789dd40d51c0d20a30d0dc44a8c07/nova/virt/ironic/driver.py#L1251-L1264">
        nova/virt/ironic/driver.py</a>. Everything else is handled by
    Ironic itself.</p>
</section>

<section>
    <h1>Initiating the deployment</h1>
    <h2>Provision state change</h2>
    <p>The deployment is initiated by requesting provision state
    <var>active</var> for the node.</p>
    <p>The a looping call is established to wait for a provision state that
    indicates either success (<var>active</var>) or a failure. It also accounts
    for a potential deletion request in the middle of a deployment:
    <a href="https://github.com/openstack/nova/blob/6c3520ac5ba789dd40d51c0d20a30d0dc44a8c07/nova/virt/ironic/driver.py#L401-L428">
        nova/virt/ironic/driver.py</a></p>
</section>

</section>
<section>

<section>
    <h1 class="title">Ironic deployment overview</h1>
</section>

<section>
    <h1>Ironic deployment overview</h1>
    <h2>Preparation</h2>
    <ol>
        <li>Plug VIFs</li>
        <li>Cache images</li>
        <li>Configure boot environment (PXE, iPXE, virtual media)</li>
        <li>Connect the provisioning network to the node</li>
        <li>Boot the ramdisk (IPA)</li>
        <li>Wait for a callback from the ramdisk</li>
    </ol>
</section>

<section>
    <h1>Ironic deployment overview</h1>
    <h2>Deployment - iSCSI method</h2>
    <ol>
        <li>Request IPA to expose the root disk as an iSCSI share</li>
        <li>Mount the resulting iSCSI share to the conductor</li>
        <li>In case of partition images - partition the target
            device</li>
        <li>Flash the instance image to the target device</li>
        <li>Write the config drive, if provided</li>
        <li>In case of partition images and local boot - install the
            bootloader on the target device</li>
        <li>Unmount the iSCSI share</li>
    </ol>
</section>

<section>
    <h1>Ironic deployment overview</h1>
    <h2>Deployment - direct method</h2>
    <ol>
        <li>In case of partition images - request IPA to partition the target
            device</li>
        <li>Request IPA to flash the instance image (fetched from a Swift
            temporary URL or an HTTP location) to the target device</li>
        <li>Request IPA to write the config drive, if provided</li>
        <li>In case of partition images and local boot - request IPA to install
            the bootloader on the target device</li>
    </ol>
</section>

<section>
    <h1>Ironic deployment overview</h1>
    <h2>Finishing</h2>
    <ol>
        <li>Request IPA to power off the machine</li>
        <li>Disconnect the provisioning network and connect tenant
            network(s)</li>
        <li>Set the boot device as requested</li>
        <li>Power on the machine</li>
    </ol>
</section>

</section>
<section>

<section>
    <h1 class="title">iSCSI-based deploy</h1>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Starting the deploy</h2>
    <ol>
        <li>The deploy starts when a conductor receives
            <var>do_node_deploy </var> RPC request.</li>
        <li>A few sanity checks are done then: <em>power</em> and
            <em>deploy</em> interface validations, and check for maintenance
            mode.</li>
        <li>The node is moved to the <var>deploying</var> provision state,
            and a new thread is lauched for the remaining actions.</li>
        <li>There, the <var>prepare</var> and <var>deploy</var> methods of
            the <em>deploy</em> interface are called.</li>
    </ol>
    <p><a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/conductor/manager.py#L549-L604">
        ironic/conductor/manager.py (1)</a></p>
    <p><a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/conductor/manager.py#L2745-L2788">
        ironic/conductor/manager.py (2)</a></p>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Preparation</h2>
    <p>The <em>deploy</em> interface <var>prepare</var> method is called in
    several cases: on deployment (or rebuilding), on <em>take over</em> and on
    <em>adopting</em> a node.</p>
    <p>In case of deployment, it
    <ol>
        <li>removes tenant networks from the node (if any)</li>
        <li>adds the provisioning network (if needed)</li>
        <li>orders the <em>boot</em> interface of the node to boot
            the deployment ramdisk</li>
    </ol>
    <p><a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/iscsi_deploy.py#L495-L505">
        ironic/drivers/modules/iscsi_deploy.py</a>.</p>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Starting the deploy</h2>
    <ol>
        <li>The actual deploy process is started with caching the instance
            (user) image on the conductor. Ironic can download it from Glance,
            as well as any HTTP(s) location.</li>
        <li>The image is (usually) converted to the "raw" format first
            to ensure it can be <var>dd</var>-ed to the target device.</li>
        <li>The node is rebooted. In the <var>prepare</var> call we already
            ensured that it will boot the deployment ramdisk.</li>
        <li>At this point, the node's provision state changes from
            <var>deploying</var> to <var>deploy wait</var>, and the conductor
            idles, awaiting a callback from the ramdisk</li>
    </ol>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>IPA start up and lookup</h2>
    <p>The deployment (also cleaning and inspection) ramdisk for Ironic is
    based on <em>Ironic Python Agent</em> (<em>IPA</em>) - Python service
    providing an HTTP API for various provisioning tasks.</p>
    <p>On start up, IPA initializes <em>hardware manager(s)</em> - plugins
    handling hardware-specific aspects of provisoning. The default
    <var>GenericHardwareManager</var> is used in most cases.</p>
    <p>Then IPA gets the Ironic API URL from the kernel boot
    arguments (supplied by the <em>boot</em> interface). It calls the
    <em>lookup</em> API endpoint to figure out the current node UUID, and a few
    optional configuration parameters.</p>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>IPA heart beats</h2>
    <p>After a successful start up, IPA waits for requests, while periodically
    polling the <em>heartbeat</em> API. Ironic assigns tasks to IPA in response
    to these heart beats.</p>
    <p>All IPA-based <em>deploy</em> interface implementations process
    heart beats in a similar way, defined in the <var>HeartbeatMixin</var>
    class:
    <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/agent_base_vendor.py#L264-L329">
        ironic/drivers/modules/agent_base_vendor.py</a>.</p>
    <p>It detects the required actions by looking at the node's provision
    state. If it's <var>deploy wait</var>, the <var>continue_deploy</var>
    method is called. This method differs between different <em>deploy</em>
    interface implementations.</p>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Finding root disk</h2>
    <ol>
        <li>Ironic requests IPA to publish the target disk via iSCSI, providing
            the complete node information:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/iscsi_deploy.py#L306-L320">
                ironic/drivers/modules/iscsi_deploy.py</a>.</li>
        <li>The IPA <em>iscsi</em> extension starts with asking the current
            <em>hardware manager</em> to pick the target device.
        <li>If <em>root device hints</em> were provided on a node, they are used:
            <a href="https://github.com/openstack/ironic-python-agent/blob/30e0da15ea7efe57fea84a56ac34b857ceb80cde/ironic_python_agent/hardware.py#L675-L704">
                ironic_python_agent/hardware.py</a>,
            <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/utils.py#L328-L417">
                ironic_lib/utils.py</a>.</li>
        <li>Otherwise, the smallest disk that is larger than 4 GiB is used:
            <a href="https://github.com/openstack/ironic-python-agent/blob/30e0da15ea7efe57fea84a56ac34b857ceb80cde/ironic_python_agent/utils.py#L290-L307">
                ironic_python_agent/utils.py</a>.</li>
    </ol>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Accessing root disk</h2>
    <ol start="5">
        <li>The chosen device is published using either <var>tgtd</var> or
            <var>LIO</var>. For CentOS/RHEL, <var>LIO</var> is used:
            <a href="https://github.com/openstack/ironic-python-agent/blob/30e0da15ea7efe57fea84a56ac34b857ceb80cde/ironic_python_agent/extensions/iscsi.py#L82-L105">
                ironic_python_agent/extensions/iscsi.py</a>.</li>
        <li>On receiving success result from IPA, the conductor mounts the
            resulting iSCSI share locally:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/deploy_utils.py#L134-L242">
                ironic/drivers/modules/deploy_utils.py</a>.</li>
        <li>Then Ironic proceeds with writing the image. It is done differently
            for partition and whole-disk images.</li>
    </ol>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Whole-disk images</h2>
    <p>In this case, Ironic only needs to copy the image and create a config
    drive:
    <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/deploy_utils.py#L416-L420">
        ironic/drivers/modules/deploy_utils.py</a>.</p>
    <ol start="8">
        <li>Image is written by using <var>dd</var>, converting it to the raw
            format, if it was not done by the conductor:
            <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/disk_utils.py#L307-L318">
                ironic_lib/disk_utils.py</a>.</li>
        <li>Then the conductor checks for a present config drive partition, and
            creates one, if missing:
            <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/disk_utils.py#L722-L817">
                ironic_lib/disk_utils.py</a>.</li>
        <li>Finally, the config drive is <var>dd</var>-ed to the resulting
            partition.</li>
    </ol>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Partition images [1]</h2>
    <p>In this case, Ironic also needs to create a partition table:
    <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/disk_utils.py#L444-L573">
        ironic_lib/disk_utils.py</a>.</p>
    <ol start="8">
        <li>All metadata on the target disk is destroyed:
            <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/disk_utils.py#L350-L381">
                ironic_lib/disk_utils.py</a>.</li>
        <li>A partition table of the requested type (MBR or GPT) is created.
            The default for UEFI is GPT, otherwise MBR is used by default:
            <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/disk_utils.py#L212-L228">
                ironic_lib/disk_utils.py</a>.</li>
    </ol>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Partition images [2]</h2>
    <ol start="10">
        <li>Then the root, swap, ephemeral and config drive partitions are
            created. The root partition goes last to allow it to be extended
            later (e.g. by <em>cloud-init</em>):
            <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/disk_utils.py#L230-L268">
                ironic_lib/disk_utils.py</a>.</li>
        <li>The swap, ephemeral (optionally) and EFI (optionally)
            partitions are formatted; the root and config drive partitions
            are populated:
            <a href="https://github.com/openstack/ironic-lib/blob/52440836bdbde05580f60caf81aafc5f5a537af5/ironic_lib/disk_utils.py#L523-L557">
                ironic_lib/disk_utils.py</a>.</li>
    </ol>
</section>

<section>
    <h1>iSCSI-based deploy</h1>
    <h2>Final steps</h2>
    <ol start="12">
        <li>If local boot is requested, the conductor instructs IPA to install
            the boot loader:
            <a href="https://github.com/openstack/ironic-python-agent/blob/30e0da15ea7efe57fea84a56ac34b857ceb80cde/ironic_python_agent/extensions/image.py#L79-L189">
                ironic_python_agent/extensions/image.py</a>.
            Also the boot device is changed to "disk".</li>
        <li>The <em>boot</em> interface <var>prepare_instance</var> is
            called.</li>
        <li>The conductor asks IPA to perform a soft reboot, unless a hard
            reboot was explicitly requested for this node:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/agent_base_vendor.py#L554-L584">
                ironic/drivers/modules/agent_base_vendor.py</a>.</li>
        <li>The node is changed from provisioning to tenant network(s) via
            the appropriate <em>network</em> interface calls.</li>
        <li>Finally, the node is rebooted. The deployment is done.</li>
    </ol>
</section>

</section>
<section>

<section>
    <h1 class="title">Networking</h1>
</section>

<section>
    <h1>Networking</h1>
    <h2>The boot interface</h2>
    <p>The <em>boot</em> interface was a relatively late addition to
    Ironic.</p>
    <p>Initially, its logic was contained in the <em>deploy</em> interface, but
    it became a cause of duplication when more boot methods (e.g. virtual
    media) were introduced.</p>
    <p>Currently, the <em>boot</em> interface is responsible for booting both
    the deployment (and cleaning) ramdisk, and the final instance.</p>
    <p><small>There is still, however, a lot of boot code in the
        <em>deploy</em> interface implementations :(</small></p>
</section>

<section>
    <h1>Networking</h1>
    <h2>PXE boot overview</h2>
    <p>The <em>pxe</em> <em>boot</em> interface is the generic boot interface
    working with (nearly) all hardware. It works by populating a PXE or iPXE
    environment for a given node.</p>
    <p>It works differently depending on</p>
    <ul>
        <li>whether PXE or iPXE is configured,</li>
        <li>whether the instance image is partition or whole-disk,</li>
        <li>whether local or network boot for the instance is requested,</li>
        <li>whether BIOS or UEFI boot is used for the node.</li>
    </ul>
</section>

<section>
    <h1>Networking</h1>
    <h2>PXE bootstrap</h2>
    <ol>
        <li>Neutron requests the node to boot the PXE ROM
            (<var>pxelinux.0</var>) from the conductor's TFTP server.</li>
        <li>The PXE ROM requests the configuration file named
            <var>pxelinux.cfg/{MAC}</var> from TFTP. This file is generated by
            the conductor:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/pxe_config.template">
                ironic/drivers/modules/pxe_config.template</a>.</li>
        <li>This configuration file boots the kernel/ramdisk pair published by
            the conductor on TFTP for the node.</li>
    </ol>
</section>

<section>
    <h1>Networking</h1>
    <h2>iPXE bootstrap</h2>
    <ol>
        <li>If the node does not indicate (in its DHCP) request that it's
            running iPXE, Neutron sends it the iPXE ROM
            (<var>undionly.kpxe</var> for BIOS, <var>ipxe.efi</var> for
            UEFI).</li>
        <li>When the node runs the iPXE ROM, it is instucted to fetch the iPXE
            script <var>boot.ipxe</var> from the conductor's HTTP server. This
            file is auto-generated and is the same for all nodes:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/boot.ipxe">
                ironic/drivers/modules/boot.ipxe</a>.</li>
        <li>This script loads another script at <var>pxelinux.cfg/{MAC}</var>
            generation by the conductor for this node:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/ipxe_config.template">
                ironic/drivers/modules/ipxe_config.template</a>.</li>
        <li>The final script boots the kernel/ramdisk pair published by
            the conductor on HTTP for the node.</li>
    </ol>
</section>

<section>
    <h1>Networking</h1>
    <h2>DHCP configuration</h2>
    <ol>
        <li>The DHCP options are generated for either PXE or iPXE boot:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/common/pxe_utils.py#L277-L316">
                ironic/common/pxe_utils.py</a>.</li>
        <li>The <var>update_dhcp_opts</var> method of a <em>DHCP provider</em>
            is called with these options. It ends up populating
            <var>extra_dhcp_opts</var> on every VIF:
            <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/dhcp/neutron.py#L37-L129">
                ironic/dhcp/neutron.py</a>.</li>
    </ol>
</section>

<section>
    <h1>Networking</h1>
    <h2>Boot configuration</h2>
    <p>For a ramdisk boot, the conductor places IPA kernel and ramdisk to the
    TFTP or HTTP location, and renders a PXE configuration file or an iPXE
    script pointing at them:
    <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/pxe.py#L410-L430">
        ironic/drivers/modules/pxe.py</a>.</p>
    <p>For instance local boot (including whole-disk images which always boot
    locally), all PXE/iPXE configuration is merely removed, and the node's
    boot device is set to "disk".</p>
    <p>For instance network boot, new PXE/iPXE configuration is written,
    pointing to instance kernel/ramdisk on a TFTP or HTTP location:
    <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/pxe.py#L469-L500">
        ironic/drivers/modules/pxe.py</a>.</p>
</section>

<section>
    <h1>Networking</h1>
    <h2>Plugging flat networks</h2>
    <p>When Ironic is used with <em>flat</em> networking, it is assumed that
    both provisioning and tenant traffic happens on the same flat network.</p>
    <p>Attaching the provisioning networks boils down to merely setting
    <var>binding:host_id</var> on all VIFs:
    <a href="https://github.com/openstack/ironic/blob/c1044b8d512a3c04d05689056a835bd7bf0b72d5/ironic/drivers/modules/network/flat.py#L56-L88">
        ironic/drivers/modules/network/flat.py</a>.</p>
    <p>Nothing is required for attaching the tenant network.</p>
</section>

</section>

			</div>
		</div>

		<script src="../reveal.js/lib/js/head.min.js"></script>
		<script src="../reveal.js/js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
                                history: true,
                                transitionSpeed: 'fast',
				dependencies: [
					{ src: '../reveal.js/plugin/markdown/marked.js' },
					{ src: '../reveal.js/plugin/markdown/markdown.js' },
					{ src: '../reveal.js/plugin/notes/notes.js', async: true },
					{ src: '../reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
